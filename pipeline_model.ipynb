{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41374ba3",
   "metadata": {},
   "source": [
    "# Presplit/deterministic enrichment\n",
    "The formated dataset still has some multi-entry columns (e.g. country). Creating a dummy for every country would lead to 171 extra columns (times the other multi-entry columns). This risks overfitting and a lot of missings. We thus group the countries in advance by their geographical location. Furthermore, SME and organizationID are multi-entry columns too. We create numerical columns to count the number of SMEs and organizations, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "006d892e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, StandardScaler, PowerTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline     import Pipeline, FeatureUnion\n",
    "from sklearn.impute       import SimpleImputer\n",
    "from sklearn.compose          import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bfc8c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"eu_data.csv\")\n",
    "\n",
    "# Define geographic groups\n",
    "geo_groups = {\n",
    "    'Western Europe': {'DE', 'FR', 'BE', 'NL', 'LU', 'CH', 'AT', 'LI'},\n",
    "    'Northern Europe': {'UK', 'IE', 'SE', 'FI', 'DK', 'IS', 'NO', 'EE', 'LV', 'LT'},\n",
    "    'Southern Europe': {'IT', 'ES', 'PT', 'EL', 'MT', 'CY', 'SI'},\n",
    "    'Eastern Europe': {'PL', 'CZ', 'SK', 'HU', 'RO', 'BG', 'RS', 'UA', 'AL', 'MK', 'ME', 'XK', 'HR', 'MD', 'GE', 'BA'},\n",
    "    'Africa': {'ZA', 'KE', 'UG', 'TN', 'GH', 'MA', 'TZ', 'EG', 'SN', 'CD', 'MZ', 'RW', 'BF', 'ZM', 'CI', 'CM', 'ET', 'NG', 'DZ', 'AO', 'GN', 'BJ', 'GA', 'MW', 'ML', 'BI', 'MU', 'ST', 'LR', 'ZW', 'CG', 'GW', 'NE', 'LY', 'GQ', 'SD', 'LS', 'TD', 'DJ'},\n",
    "    'Asia': {'IL', 'TR', 'IN', 'CN', 'JP', 'KR', 'TH', 'SG', 'LB', 'TW', 'UZ', 'AM', 'VN', 'MY', 'KZ', 'PK', 'AZ', 'HK', 'ID', 'JO', 'BD', 'KG', 'IR', 'PS', 'MN', 'KH', 'TJ', 'IQ', 'TM', 'NP', 'KW', 'QA', 'AF', 'BT', 'MO', 'MV', 'LA', 'LK'},\n",
    "    'Oceania': {'AU', 'NZ', 'FJ', 'MH', 'PG', 'NC'},\n",
    "    'Americas': {'US', 'CA', 'BR', 'AR', 'CO', 'CL', 'MX', 'PE', 'UY', 'BO', 'CR', 'PA', 'GT', 'SV', 'PY', 'EC', 'VE', 'DO', 'HT', 'SR', 'AW', 'BQ', 'AI', 'GU'}\n",
    "}\n",
    "\n",
    "# Number of partners from geo_groups \n",
    "for region, countries in geo_groups.items():\n",
    "    df[f'{region}_count'] = (\n",
    "        df['country']\n",
    "          .str.split(';')  \n",
    "          .apply(lambda lst: sum(c.strip() in countries for c in lst))\n",
    "    )\n",
    "\n",
    "# Number of countries per project\n",
    "df['num_countries'] = df['country'].str.split(';').apply(lambda x: len([c.strip() for c in x if c.strip()]))\n",
    "\n",
    "# Number of organizations per project (later choose only one of the 2 as feature because almost the same)\n",
    "df['num_organisations'] = df['organisationID'].str.split(';').apply(lambda x: len([o.strip() for o in x if o.strip()]))\n",
    "\n",
    "# Number of SMEs per project\n",
    "def count_smes(s):\n",
    "    entries = [e.strip() for e in s.split(';') if e.strip()]\n",
    "    return sum(1 for e in entries if e.lower() == 'true')\n",
    "\n",
    "df['num_sme'] = df['SME'].apply(count_smes)\n",
    "\n",
    "# Number of different roles per project\n",
    "df['role_list'] = df['role'].str.split(';').apply(lambda L: [r.strip() for r in L])\n",
    "df['n_participant'] = df['role_list'].apply(lambda L: L.count('participant'))\n",
    "df['n_associatedPartner']= df['role_list'].apply(lambda L: L.count('associatedPartner'))\n",
    "df['n_thirdParty']= df['role_list'].apply(lambda L: L.count('thirdParty'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f86f413",
   "metadata": {},
   "source": [
    "We also want to transform the date columns into the more meaningful variables: starting year, starting month and duration, signature lag days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c489ab67",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['startDate', 'endDate', 'ecSignatureDate']:\n",
    "    df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    \n",
    "df['duration_days']      = (df.endDate - df.startDate).dt.days\n",
    "df['start_year']         = df.startDate.dt.year\n",
    "df['start_month']        = df.startDate.dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffd7d99",
   "metadata": {},
   "source": [
    "The EuroSciVoc table has additional information about the topic domain. We want to include that in our data and analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "031c08cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   projectID        acronym  status  \\\n",
      "0  101159220      PvSeroRDT  SIGNED   \n",
      "1  101096150       BIOBoost  SIGNED   \n",
      "2  101093997  GlycanTrigger  SIGNED   \n",
      "3  101126531   CHIKVAX_CHIM  SIGNED   \n",
      "4  101113979      The Oater  CLOSED   \n",
      "\n",
      "                                               title  startDate    endDate  \\\n",
      "0  A point-of-care serological rapid diagnostic t... 2025-02-01 2030-01-31   \n",
      "1  Boosting innovation agencies for bioeconomy va... 2023-02-01 2025-01-31   \n",
      "2  GLYCANS AS MASTER TRIGGERS OF HEALTH TO INTEST... 2023-01-01 2028-12-31   \n",
      "3  Late-stage clinical development of Chikungunya... 2023-06-01 2028-11-30   \n",
      "4  The Oater develops a compact machine for hyper... 2023-07-01 2023-12-31   \n",
      "\n",
      "   ecMaxContribution ecSignatureDate                              masterCall  \\\n",
      "0         4062396.23      2024-12-09  HORIZON-JU-GH-EDCTP3-2023-02-two-stage   \n",
      "1          500000.00      2022-11-25             HORIZON-EIE-2022-CONNECT-01   \n",
      "2         6771571.00      2022-12-05           HORIZON-HLTH-2022-STAYHLTH-02   \n",
      "3        70000000.00      2023-06-15           HORIZON-HLTH-2022-CEPI-15-IBA   \n",
      "4           75000.00      2023-06-05             HORIZON-EIE-2022-SCALEUP-02   \n",
      "\n",
      "    fundingScheme  ... num_organisations num_sme  \\\n",
      "0  HORIZON-JU-RIA  ...                 8       1   \n",
      "1     HORIZON-CSA  ...                 8       3   \n",
      "2     HORIZON-RIA  ...                10       2   \n",
      "3  HORIZON-COFUND  ...                 1       0   \n",
      "4     HORIZON-CSA  ...                 1       1   \n",
      "\n",
      "                                           role_list n_participant  \\\n",
      "0  [associatedPartner, participant, participant, ...             2   \n",
      "1  [participant, coordinator, participant, partic...             7   \n",
      "2  [participant, associatedPartner, coordinator, ...             7   \n",
      "3                                      [coordinator]             0   \n",
      "4                                      [coordinator]             0   \n",
      "\n",
      "  n_associatedPartner n_thirdParty duration_days start_year start_month  \\\n",
      "0                   5            0          1825       2025           2   \n",
      "1                   0            0           730       2023           2   \n",
      "2                   1            1          2191       2023           1   \n",
      "3                   0            0          2009       2023           6   \n",
      "4                   0            0           183       2023           7   \n",
      "\n",
      "               euroSciVoxTopic  \n",
      "0  medical and health sciences  \n",
      "1                not available  \n",
      "2  medical and health sciences  \n",
      "3             natural sciences  \n",
      "4             natural sciences  \n",
      "\n",
      "[5 rows x 46 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the Euroscivox data\n",
    "df2 = pd.read_csv(\"EuroSciVox.csv\", usecols=['projectID','euroSciVocPath'])\n",
    "\n",
    "# Extract the first “folder” from the path as topic domain (the thing between the first two slashes)\n",
    "df2['euroSciVoxTopic'] = df2['euroSciVocPath'].str.extract(r'^/([^/]+)/?')\n",
    "\n",
    "# Keep one row per projectID\n",
    "df2 = (\n",
    "    df2[['projectID','euroSciVoxTopic']]\n",
    "      .drop_duplicates(subset='projectID')\n",
    ")\n",
    "\n",
    "# Left‐merge into main df\n",
    "df = df.merge(df2, on='projectID', how='left')\n",
    "\n",
    "df['euroSciVoxTopic'] = df['euroSciVoxTopic'].fillna('not available')\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6118bce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"df_eu.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4855675f",
   "metadata": {},
   "source": [
    "# Feature Enrichment and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71d92ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) train/test split\n",
    "target = df['ecMaxContribution']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df, target, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15534f3",
   "metadata": {},
   "source": [
    "We have the organisationID column and also the net_ecContribution_list column. We want to include a feature for each organisation’s “past mean EC” using only contributions from earlier projects (shifting the cumulative sum/count so the current row’s own contribution is excluded). In addition, we need a lookup (org_dim) that the transformer uses at predict time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d9ce106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) precompute lookup table\n",
    "# organization lookup\n",
    "train_long = (\n",
    "    X_train[['projectID','organisationID','ecContribution_list','startDate']]\n",
    "    .copy() \n",
    ")\n",
    "\n",
    "# normalize delimiters\n",
    "train_long['clean_orgs'] = (\n",
    "    train_long['organisationID']\n",
    "      .astype(str)\n",
    "      .str.strip('[]')\n",
    "      .str.replace(r'[;,]', ';', regex=True)\n",
    ")\n",
    "\n",
    "train_long['clean_ecs'] = (\n",
    "    train_long['ecContribution_list']\n",
    "      .astype(str)\n",
    "      .str.strip('[]')\n",
    "      .str.replace(r'[;,]', ';', regex=True)\n",
    ")\n",
    "\n",
    "# split into lists\n",
    "train_long['org_list'] = train_long['clean_orgs'].str.split(';').apply(lambda lst: [o.strip() for o in lst if o.strip()])\n",
    "train_long['ec_list']  = train_long['clean_ecs'].str.split(';').apply(lambda lst: [float(x)       for x in lst if x.strip()])\n",
    "\n",
    "# explode\n",
    "train_long['pairs'] = train_long.apply(lambda r: list(zip(r['org_list'], r['ec_list'])), axis=1)\n",
    "train_long = train_long.explode('pairs')\n",
    "train_long[['organisationID','ecContribution']] = pd.DataFrame(train_long['pairs'].tolist(), index=train_long.index)\n",
    "\n",
    "# compute each org’s historical mean (excluding current)\n",
    "train_long = train_long.sort_values(['organisationID','startDate'])\n",
    "train_long['cum_sum']   = train_long.groupby('organisationID')['ecContribution'].cumsum().shift(1).fillna(0)\n",
    "train_long['cum_count'] = train_long.groupby('organisationID')['ecContribution'].cumcount()\n",
    "train_long['past_mean'] = (train_long['cum_sum'] / train_long['cum_count'].replace(0,np.nan)).fillna(0)\n",
    "\n",
    "# build org_dim\n",
    "org_dim = (\n",
    "    train_long\n",
    "      .groupby('organisationID')['past_mean']\n",
    "      .last()\n",
    "      .reset_index()\n",
    "      .rename(columns={'past_mean':'org_past_mean_ec'})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e484e3f",
   "metadata": {},
   "source": [
    "We have textual features such as the objective column. We use LDA for topic (objective) modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd5ba248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective #1: materials energy high devices low technology storage power co2 performance\n",
      "Objective #2: training scientific researchers science skills european university knowledge academic expertise\n",
      "Objective #3: data european eu climate support policy innovation solutions stakeholders digital\n",
      "Objective #4: protein plant drug potential molecular novel chemical proteins develop synthetic\n",
      "Objective #5: cell cells cancer disease patients clinical treatment brain tissue mechanisms\n",
      "Objective #6: energy technology production market technologies industrial industry cost design sustainable\n",
      "Objective #7: understanding role change global processes climate changes knowledge evolution environmental\n",
      "Objective #8: social cultural political studies history heritage analysis practices gender historical\n",
      "Objective #9: quantum systems time data models high physics methods field theory\n",
      "Objective #10: data health ai learning human social risk care people life\n",
      "      projectID              acronym  status  \\\n",
      "2333  101061171  STDP-development-AD  SIGNED   \n",
      "6461  101161573               HiSOPE  SIGNED   \n",
      "875   101103095     Trans Modernismo  CLOSED   \n",
      "4848  101114853                 MAIA  SIGNED   \n",
      "2945  101155227            KetoFAPIs  SIGNED   \n",
      "\n",
      "                                                  title  startDate    endDate  \\\n",
      "2333  Neuronal networks dynamics underlying spike ti... 2023-01-01 2025-04-30   \n",
      "6461    High-Speed Organic Photonics and OptoElecronics 2024-11-01 2028-10-31   \n",
      "875   Birth of the Marimacho: Modernismo’s Trans* Cu... 2023-09-01 2025-08-31   \n",
      "4848         Multimodal Access for Intelligent Airports 2023-06-01 2025-11-30   \n",
      "2945  Next generation ketoamide inhibitors of fibrob... 2024-11-01 2026-10-31   \n",
      "\n",
      "      ecMaxContribution ecSignatureDate  \\\n",
      "2333          193402.26      2022-11-03   \n",
      "6461         3066939.00      2024-09-11   \n",
      "875           173847.36      2023-04-05   \n",
      "4848         1000000.00      2023-05-23   \n",
      "2945          191760.00      2024-10-01   \n",
      "\n",
      "                                    masterCall           fundingScheme  ...  \\\n",
      "2333                   HORIZON-MSCA-2021-PF-01  HORIZON-TMA-MSCA-PF-EF  ...   \n",
      "6461  HORIZON-EIC-2023-PATHFINDERCHALLENGES-01             HORIZON-EIC  ...   \n",
      "875                    HORIZON-MSCA-2022-PF-01  HORIZON-TMA-MSCA-PF-EF  ...   \n",
      "4848              HORIZON-SESAR-2022-DES-ER-01          HORIZON-JU-RIA  ...   \n",
      "2945                   HORIZON-MSCA-2023-PF-01  HORIZON-TMA-MSCA-PF-EF  ...   \n",
      "\n",
      "     num_organisations num_sme  \\\n",
      "2333                 1       0   \n",
      "6461                 7       0   \n",
      "875                  1       0   \n",
      "4848                 6       4   \n",
      "2945                 1       0   \n",
      "\n",
      "                                              role_list n_participant  \\\n",
      "2333                                      [coordinator]             0   \n",
      "6461  [thirdParty, participant, associatedPartner, a...             3   \n",
      "875                                       [coordinator]             0   \n",
      "4848  [participant, participant, participant, partic...             5   \n",
      "2945                                      [coordinator]             0   \n",
      "\n",
      "     n_associatedPartner n_thirdParty duration_days start_year start_month  \\\n",
      "2333                   0            0           850       2023           1   \n",
      "6461                   2            1          1460       2024          11   \n",
      "875                    0            0           730       2023           9   \n",
      "4848                   0            0           913       2023           6   \n",
      "2945                   0            0           729       2024          11   \n",
      "\n",
      "                  euroSciVoxTopic  \n",
      "2333             natural sciences  \n",
      "6461             natural sciences  \n",
      "875               social sciences  \n",
      "4848   engineering and technology  \n",
      "2945  medical and health sciences  \n",
      "\n",
      "[5 rows x 46 columns]\n"
     ]
    }
   ],
   "source": [
    "# 3) Build & fit LDA pipeline on training “objective” text\n",
    "\n",
    "# define extra stop-words\n",
    "extra_stops = {'project', 'new', 'study', 'research', 'based', 'use'}\n",
    "stop_words = list(ENGLISH_STOP_WORDS.union(extra_stops))\n",
    "\n",
    "# LDA pipeline\n",
    "lda_pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "        stop_words=stop_words,\n",
    "        min_df=5,\n",
    "        ngram_range=(1,2)\n",
    "    )),\n",
    "    ('lda', LatentDirichletAllocation(\n",
    "        n_components=10,\n",
    "        max_iter=20,\n",
    "        learning_method='online',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )),\n",
    "])\n",
    "\n",
    "# fit on train objectives\n",
    "lda_pipeline.fit(X_train['objective'])\n",
    "\n",
    "# top words per topic\n",
    "def display_topics(model, feature_names, n_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        print(f\"Objective #{topic_idx + 1}: {' '.join(top_features)}\")\n",
    "\n",
    "# display topics\n",
    "vectorizer = lda_pipeline.named_steps['vect']\n",
    "lda_model  = lda_pipeline.named_steps['lda']\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "display_topics(lda_model, feature_names, n_top_words=10)\n",
    "\n",
    "# objective‐weight matrix\n",
    "doc_obj_dist = lda_pipeline.transform(X_train['objective'])\n",
    "\n",
    "# dataframe with generic names\n",
    "obj_cols = [f\"obj_{i+1}\" for i in range(doc_obj_dist.shape[1])]\n",
    "objective_df = pd.DataFrame(\n",
    "    doc_obj_dist,\n",
    "    columns=obj_cols,\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "# descriptive names\n",
    "new_names = {\n",
    "    'obj_1':  'obj_advanced_energy_storage_materials',\n",
    "    'obj_2':  'obj_researcher_academic_training',\n",
    "    'obj_3':  'obj_eu_climate_policy_data',\n",
    "    'obj_4':  'obj_molecular_synthetic_biology',\n",
    "    'obj_5':  'obj_clinical_cell_cancer_biology',\n",
    "    'obj_6':  'obj_industrial_sustainable_energy',\n",
    "    'obj_7':  'obj_global_environmental_change',\n",
    "    'obj_8':  'obj_social_cultural_studies',\n",
    "    'obj_9':  'obj_quantum_theoretical_physics',\n",
    "    'obj_10': 'obj_digital_health_ai'\n",
    "}\n",
    "\n",
    "objective_df = objective_df.rename(columns=new_names)\n",
    "train_df = X_train.join(objective_df)\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2056f308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5a) wrap OrgAvgPastEC so that it _only_ returns its new column:\n",
    "class OrgAvgPastEC(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, org_dim):\n",
    "        # org_dim: DataFrame(org, mean) or dict\n",
    "        if hasattr(org_dim, 'set_index'):\n",
    "            org_dim = org_dim.set_index('organisationID')['org_past_mean_ec'].to_dict()\n",
    "        self.org_dim = org_dim\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # X: DataFrame with at least projectID, organisationID, ecContribution_list\n",
    "        proj_to_means = {}\n",
    "        for _, row in X.iterrows():\n",
    "            pid = row['projectID']\n",
    "            orgs = (\n",
    "                str(row['organisationID'])\n",
    "                   .strip('[]')\n",
    "                   .replace(r',', ';')\n",
    "                   .split(';')\n",
    "            )\n",
    "            orgs = [o.strip() for o in orgs if o.strip()]\n",
    "            means = [ self.org_dim.get(o, 0.0) for o in orgs ]\n",
    "            proj_to_means[pid] = float(np.mean(means)) if means else 0.0\n",
    "\n",
    "        arr = X['projectID'].map(proj_to_means).fillna(0.0).values\n",
    "        return arr.reshape(-1,1)\n",
    "\n",
    "\n",
    "# 5b) wrap ObjectiveMeanLookup similarly\n",
    "class ObjectiveMeanEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lda_pipeline):\n",
    "        self.lda_pipeline = lda_pipeline\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        doc_topic = self.lda_pipeline.transform(X['objective'])\n",
    "        y_arr     = np.array(y).reshape(-1,1)\n",
    "        sums      = (doc_topic * y_arr).sum(axis=0)\n",
    "        weights   = doc_topic.sum(axis=0)\n",
    "        self.topic_means_ = sums / weights\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        doc_topic = self.lda_pipeline.transform(X['objective'])\n",
    "        hist_mean = (doc_topic * self.topic_means_).sum(axis=1)\n",
    "        return hist_mean.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd79662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Short exploration of numerical variables (distribution and boxplots)\n",
    "numeric_cols = [\n",
    "    'ecMaxContribution','duration_days','n_participant','n_associatedPartner',\n",
    "    'n_thirdParty', 'Western Europe_count', 'Eastern Europe_count', 'Northern Europe_count',\n",
    "    'Southern Europe_count', 'Africa_count', 'Asia_count', 'Oceania_count', 'Americas_count',\n",
    "    'num_countries', 'num_organisations', 'num_sme', 'start_year']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    plt.figure()\n",
    "    plt.hist(X_train[col].dropna(), bins=30)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "X_train[numeric_cols].boxplot(rot=45)\n",
    "plt.title(\"Boxplots of Key Numeric Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5f28f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Preprocess numerical and categorical features\n",
    "# Custom target-transformer: winsorize at 1st/99th percentile, then log1p\n",
    "\n",
    "class WinsorLogTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lower=0.01, upper=0.99):\n",
    "        self.lower = lower\n",
    "        self.upper = upper\n",
    "\n",
    "    def fit(self, y, X=None):\n",
    "        y = np.array(y).ravel()\n",
    "        self._clip_lo, self._clip_hi = np.quantile(y, [self.lower, self.upper])\n",
    "        return self\n",
    "\n",
    "    def transform(self, y):\n",
    "        y = np.array(y).ravel()\n",
    "        y = np.clip(y, self._clip_lo, self._clip_hi)\n",
    "        return np.log1p(y).reshape(-1, 1)\n",
    "\n",
    "    def inverse_transform(self, y):\n",
    "        return np.expm1(y)\n",
    "\n",
    "target_transformer = WinsorLogTransformer(lower=0.01, upper=0.99)\n",
    "\n",
    "# Cyclical encoder for month\n",
    "class CyclicalEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, col, period):\n",
    "        self.col = col\n",
    "        self.period = period\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        vals = X[self.col].astype(float).values\n",
    "        sin = np.sin(2 * np.pi * vals / self.period)\n",
    "        cos = np.cos(2 * np.pi * vals / self.period)\n",
    "        return np.vstack([sin, cos]).T\n",
    "\n",
    "\n",
    "month_encoder = Pipeline([\n",
    "    ('cycle', CyclicalEncoder(col='start_month', period=12)),\n",
    "    ('scale', StandardScaler()),\n",
    "])\n",
    "\n",
    "# numeric & categorical pipelines\n",
    "num_pipe = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='median')),\n",
    "    ('scale',  StandardScaler()),\n",
    "])\n",
    "\n",
    "cat_cols = ['fundingScheme', 'masterCall', 'euroSciVoxTopic']\n",
    "cat_pipe = Pipeline([\n",
    "    ('impute', OrdinalEncoder()),  \n",
    "])\n",
    "\n",
    "# Put it all together in a FeatureUnion\n",
    "preprocessor = FeatureUnion([\n",
    "    ('numeric',      ColumnTransformer([('num', num_pipe, numeric_cols)], remainder='drop')),\n",
    "    ('categorical',  ColumnTransformer([('cat', cat_pipe, cat_cols)], remainder='drop')),\n",
    "    ('month_cycle',  month_encoder),\n",
    "    ('topic_enc', topic_encoder),\n",
    "    ('org_avg_ec',   Pipeline([\n",
    "                         ('org_avg', OrgAvgPastEC(org_dim)),\n",
    "                         ('scale',   StandardScaler())\n",
    "                     ])),\n",
    "    ('obj_mean_ec',  ObjectiveMeanEncoder(lda_pipeline)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ec9186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedLDATopicEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,\n",
    "                 n_components=10,\n",
    "                 vectorizer_params=None,\n",
    "                 lda_params=None):\n",
    "        self.n_components     = n_components\n",
    "        self.vectorizer_params = vectorizer_params or {}\n",
    "        self.lda_params        = lda_params or {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        texts = X['objective']\n",
    "\n",
    "        # 1) Fit the vectorizer on this fold\n",
    "        self.vectorizer_ = CountVectorizer(**self.vectorizer_params)\n",
    "        dtm = self.vectorizer_.fit_transform(texts)\n",
    "\n",
    "        # 2) Fit the LDA on that DTM\n",
    "        self.lda_ = LatentDirichletAllocation(\n",
    "            n_components=self.n_components,\n",
    "            **self.lda_params\n",
    "        ).fit(dtm)\n",
    "\n",
    "        # 3) Compute fold‐specific topic_means\n",
    "        doc_topic = self.lda_.transform(dtm)       # shape = (n_samples, n_topics)\n",
    "        y_arr     = np.array(y).reshape(-1,1)\n",
    "        sums      = (doc_topic * y_arr).sum(axis=0)\n",
    "        weights   = doc_topic.sum(axis=0)\n",
    "        self.topic_means_ = sums / weights\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        dtm       = self.vectorizer_.transform(X['objective'])\n",
    "        doc_topic = self.lda_.transform(dtm)\n",
    "        # return the weighted sum per doc\n",
    "        return (doc_topic * self.topic_means_).sum(axis=1).reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7840375",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_encoder = SupervisedLDATopicEncoder(\n",
    "    n_components=10,\n",
    "    vectorizer_params={\n",
    "      'stop_words': stop_words,\n",
    "      'min_df':     5,\n",
    "      'ngram_range': (1,2)\n",
    "    },\n",
    "    lda_params={\n",
    "      'max_iter':       20,\n",
    "      'learning_method': 'online',\n",
    "      'random_state':    42,\n",
    "      'n_jobs':          -1\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc07d1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n"
     ]
    },
    {
     "ename": "TerminatedWorkerError",
     "evalue": "A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTerminatedWorkerError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 42\u001b[0m\n\u001b[0;32m     31\u001b[0m search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[0;32m     32\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mpipeline,\n\u001b[0;32m     33\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     return_train_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     39\u001b[0m )\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Fit the search on train data\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[43msearch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest R² (CV):\u001b[39m\u001b[38;5;124m\"\u001b[39m, search\u001b[38;5;241m.\u001b[39mbest_score_)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest params:\u001b[39m\u001b[38;5;124m\"\u001b[39m, search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32mc:\\Users\\Nutzer\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nutzer\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Nutzer\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1419\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1419\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nutzer\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Nutzer\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nutzer\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nutzer\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Nutzer\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\joblib\\parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1748\u001b[0m \n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1754\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Nutzer\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\joblib\\parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1789\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nutzer\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\joblib\\parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Nutzer\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\joblib\\parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mTerminatedWorkerError\u001b[0m: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV  # or RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# 7) Train model\n",
    "base_pipeline = Pipeline([\n",
    "    ('features', preprocessor),\n",
    "    ('model',    XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        random_state=42,\n",
    "        verbosity=1,          \n",
    "        tree_method='hist'    \n",
    "    )),\n",
    "])\n",
    "\n",
    "pipeline = TransformedTargetRegressor(\n",
    "    regressor=base_pipeline,\n",
    "    transformer=target_transformer\n",
    ")\n",
    "\n",
    "# Parameter grid\n",
    "param_grid = {\n",
    "    'regressor__model__n_estimators':    [100, 300, 600],\n",
    "    'regressor__model__max_depth':       [3, 6, 10],\n",
    "    'regressor__model__learning_rate':   [0.01, 0.1, 0.2],\n",
    "    'regressor__model__subsample':       [0.7, 1.0],\n",
    "    'regressor__model__colsample_bytree': [0.6, 0.8, 1.0],\n",
    "}\n",
    "\n",
    "# search\n",
    "search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=2,           \n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Fit the search on train data\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best R² (CV):\", search.best_score_)\n",
    "print(\"Best params:\", search.best_params_)\n",
    "\n",
    "# Evaluate\n",
    "test_r2 = search.score(X_test, y_test)\n",
    "print(\"Test R² with best model:\", test_r2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
